# Working with LLMs

Notebooks research on LLM capacities, including running autonomous LLMs locally *(Vicuna, GPT4all)* or using OpenAI ChatGPT API.

*Langchain_external_data_to_LLM.ipynb*      Exploring LangChain: some use cases how to connect external data to LLMs

*Embedchain_chatbot_on_external_data.ipynb*         Simple chatbot connecting external data with LLM () in two steps: embedding the data for vector database (several options are available), and interaction with vector database using LLM capacities (LLMs: OpenAI, VertexAI, GPT4all, Azure_ChatGPT etc.) 

*LLM_LoRA_finetuning_on_external_data.ipynb*        Training a Causal Language Model on external sensitive data using LoRA technique

*try_GPT4all_locally.ipynb*             Working with local LLMs of *GPT4All* family (using Python wrapper)  

*try_LLaMa_locally.ipynb*           Notebook Chat session with local LLM using llama.cpp engine

*try_LLaMa_locally.py*       Script for chating with local LLM using llama.cpp engine
