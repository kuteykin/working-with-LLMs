{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Causal Language Model on external sensitive data using LoRA technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (will be done autonomously on order to avoid data leakage)\n",
    "## LLM = BigScience Bloom-3b (*open-source*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset_path: Path, min_length: int, tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"Prepare dataset for training from the jsonl file.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (Path): Extracted text from the book\n",
    "        min_length (int): Filter pages without text\n",
    "        tokenizer (PreTrainedTokenizer): HuggingFace tokenizer\n",
    "\n",
    "    Yields:\n",
    "        str: text of the pages\n",
    "    \"\"\"\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        grouped_text = \"\"\n",
    "        for line in f:\n",
    "            elt = json.loads(line)\n",
    "            text: str = list(elt.values())[0]\n",
    "            if len(text) > min_length:\n",
    "                grouped_text += text\n",
    "        # End of paragraphs defined by \".\\n is transformed into EOS token\"\n",
    "        grouped_text = grouped_text.replace(\".\\n\", \".\" + tokenizer.eos_token)\n",
    "        return preprocess_text(grouped_text)S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element: Mapping, tokenizer: Callable, \n",
    "             context_length: int) -> str:\n",
    "    inputs = tokenizer(element['text'], truncation=True, return_overflowing_tokens=True, \n",
    "                       return_length=True, max_length=context_length)\n",
    "    inputs_batch = []\n",
    "    for length, input_ids in zip(inputs['length'], inputs['input_ids']):\n",
    "        if length == context_length: # We drop the last input_ids that are shorter than max_length\n",
    "            inputs_batch.append(input_ids)\n",
    "    return {\"input_ids\": inputs_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_path: Path, min_length: int, context_length: int, \n",
    "                    test_size: float, shuffle: bool, hf_repo: str) -> None:\n",
    "    \"\"\"Prepare dataset for training and push it to the hub.\n",
    "    \"\"\"\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "    LOGGER.info(f'Start preparing dataset from {dataset_path}')\n",
    "    text = preprocess_data(dataset_path=dataset_path, min_length=min_length, tokenizer=tokenizer)\n",
    "    dataset = Dataset.from_dict({'text': [text]})\n",
    "    tokenized_dataset = dataset.map(tokenize, batched=True, fn_kwargs={'tokenizer': tokenizer, 'context_length': context_length},\n",
    "                                         remove_columns=dataset.column_names)\n",
    "    LOGGER.info(f'The tokenized dataset is composed of {tokenized_dataset.num_rows} elements, each one composed of {context_length} tokens.')\n",
    "    tokenized_dataset_dict = tokenized_dataset.train_test_split(test_size=test_size, shuffle=shuffle)\n",
    "    LOGGER.info(f'The training dataset is composed of {tokenized_dataset_dict[\"train\"].num_rows} elements, the test dataset is composed of {tokenized_dataset_dict[\"test\"].num_rows} elements.')\n",
    "    tokenized_dataset_dict.push_to_hub(hf_repo)\n",
    "    LOGGER.info(f'Preparing dataset finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Causal language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes\n",
    "import accelerate\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\", \n",
    "                  device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if param.ndim == 1:\n",
    "  # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "  param.data = param.data.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): \n",
    "        return super().forward(x).to(float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "\n",
    "hf_repo = \"\"\n",
    "dataset = load_dataset(hf_repo)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.1,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4, \n",
    "        fp16=True,\n",
    "        logging_steps=1, \n",
    "        output_dir=\"outputs\"\n",
    "    )\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence warnings\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(hf_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "# Import the model\n",
    "config = PeftConfig.from_pretrained(hf_repo)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, hf_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The hobbits were so suprised seeing their friend\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "tokens = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_llms-G7wTTztB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
